{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93e22f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load necessary libraries\n",
    "import IPython.display as ipd\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import shuffle\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, Conv2D, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "import librosa\n",
    "import librosa.display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b459fb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'C:/Users/bupes/OneDrive/Desktop/project/genres_original/blues/blues.00000.wav'\n",
    "plt.figure(figsize=(14,5))\n",
    "data,sample_rate=librosa.load(filename)\n",
    "librosa.display.waveshow(data,sr=sample_rate)\n",
    "ipd.Audio(filename) #To show which audio file we are playing\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Audio Waveform [Blues]')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b524146f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.io import wavfile as wav\n",
    "wave_sample_rate,wave_audio = wav.read(filename)\n",
    "print(\"The sample rate is \",sample_rate)\n",
    "print(\"The sample rate using the scipy library is \",wave_sample_rate)\n",
    "print(\"The librosa data is \",data)\n",
    "print(\"The scipy data is \",wave_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20aa9da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To check if the dataset is balanced or not\n",
    "import pandas as pd\n",
    "metadata = pd.read_csv('C:/Users/bupes/OneDrive/Desktop/project/features_30_sec.csv')\n",
    "metadata.head(10)\n",
    "metadata['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230fbe3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,4))\n",
    "plt.plot(wave_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2701015",
   "metadata": {},
   "outputs": [],
   "source": [
    "header = 'filename chroma_stft rmse spectral_centroid spectral_bandwidth rolloff zero_crossing_rate tempo flux contrast flatness'\n",
    "for i in range(1, 21):\n",
    "    header += f' mfcc{i}'\n",
    "header += ' label'\n",
    "header = header.split()\n",
    "file = open('data.csv', 'w', newline='')\n",
    "with file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(header)\n",
    "genres = 'blues classical country disco hiphop jazz metal pop reggae rock'.split()\n",
    "for g in genres:\n",
    "    for filename in os.listdir('C:/Users/bupes/OneDrive/Desktop/project/genres_original/'+g):\n",
    "        songname = 'C:/Users/bupes/OneDrive/Desktop/project/genres_original/'+ g + '/' + filename\n",
    "        y, sr = librosa.load(songname, mono=True, duration=30)\n",
    "        chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "        spec_cent = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "        spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
    "        rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
    "        zcr = librosa.feature.zero_crossing_rate(y)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr)\n",
    "        #rmse = librosa.feature.mfcc(y=y)\n",
    "\n",
    "        #tempo = librosa.beat.tempo(y, sr=sr)[0]\n",
    "\n",
    "        #flux = librosa.onset.onset_strength(y=y, sr=sr)\n",
    "        #contrast = librosa.feature.spectral_contrast(y, sr=sr)\n",
    "        #flatness = librosa.feature.spectral_flatness(y)\n",
    "\n",
    "        to_append = f'{filename} {np.mean(chroma_stft)} {np.mean(spec_cent)} {np.mean(spec_bw)} {np.mean(rolloff)} {np.mean(zcr)}'    \n",
    "        for e in mfcc:\n",
    "            to_append += f' {np.mean(e)}'\n",
    "        to_append += f' {g}'\n",
    "        file = open('data.csv', 'a', newline='')\n",
    "        with file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(to_append.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c0fff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "  #spectrogram for Blues genre\n",
    "  songname = 'C:/Users/bupes/OneDrive/Desktop/project/genres_original/blues/blues.00009.wav'\n",
    "  y, sr = librosa.load(songname, mono=True, duration=30)\n",
    "  X = librosa.stft(y)\n",
    "  Xdb = librosa.amplitude_to_db(abs(X))\n",
    "  plt.figure(figsize=(14, 5))\n",
    "  librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='hz')\n",
    "  plt.colorbar()\n",
    "  print(\"Blues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099373bd",
   "metadata": {},
   "source": [
    "# Loading the saved dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7562b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9aeecf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The shape of the dataframe is\",df.shape)\n",
    "print(df.head(5))\n",
    "print(type(df))\n",
    "df = df.dropna(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e8ef77",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df = df.iloc[:,1:-1]\n",
    "y_df = df.iloc[:,-1]\n",
    "#y_df_knn = y_df_knn.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990823cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afd7091",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a1d7cf",
   "metadata": {},
   "source": [
    "# Train-Test split for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d2ac94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_df, y_df, test_size = 0.10, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0ec665",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92e549a",
   "metadata": {},
   "source": [
    "Different Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2c32ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear \n",
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'rbf', random_state = 0)\n",
    "classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e0f0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17107986",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"The accuracy score of prediction with SVM Model is \",str((accuracy*100).round(2))+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ecec58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rbf Kernal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846c8051",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier2 = SVC(kernel = 'rbf', random_state = 0)\n",
    "classifier2.fit(x_train, y_train)\n",
    "y_pred2 = classifier2.predict(x_test)\n",
    "accuracy2 = accuracy_score(y_test, y_pred2)\n",
    "print(\"The accuracy score of prediction with SVM Model is \",str((accuracy2*100).round(2))+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b05096d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#poly kernal\n",
    "classifier1 = SVC(kernel = 'poly', random_state = 0)\n",
    "classifier1.fit(x_train, y_train)\n",
    "y_pred1 = classifier1.predict(x_test)\n",
    "accuracy1 = accuracy_score(y_test, y_pred1)\n",
    "print(\"The accuracy score of prediction with SVM Model is \",str((accuracy1*100).round(2))+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c5e9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sigmoid Kernal\n",
    "classifier3 = SVC(kernel = 'sigmoid', random_state = 0)\n",
    "classifier3.fit(x_train, y_train)\n",
    "y_pred3 = classifier3.predict(x_test)\n",
    "accuracy3 = accuracy_score(y_test, y_pred3)\n",
    "print(\"The accuracy score of prediction with SVM Model is \",str((accuracy3*100).round(2))+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3a6852",
   "metadata": {},
   "outputs": [],
   "source": [
    "##simple feedforward neural network \n",
    "\n",
    "# Label encoding for y_train\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "\n",
    "# Example using TensorFlow/Keras for a simple neural network\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Determine the input shape\n",
    "your_input_shape = x_train.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_shape=(your_input_shape,)))\n",
    "model.add(Dense(10, activation='softmax'))  # Assuming 10 genres\n",
    "# Label encoding for y_test\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train_encoded, epochs=10, batch_size=32, validation_data=(x_test, y_test_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac0efb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test_encoded)\n",
    "print(f'Test Loss: {test_loss}, Test Accuracy: {test_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdc017b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simpler implementation of a CNN with basic training and evaluation\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Assuming x_train and x_test are already defined\n",
    "# Assuming y_train and y_test are already defined\n",
    "\n",
    "# Label encoding for y_train and y_test\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Reshape the data for CNN\n",
    "x_train_cnn = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))  # Adjusted input shape\n",
    "x_test_cnn = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))\n",
    "\n",
    "# CNN model architecture\n",
    "model_cnn = Sequential()\n",
    "model_cnn.add(Conv1D(32, kernel_size=3, activation='relu', input_shape=(x_train.shape[1], 1)))  # Adjusted input shape\n",
    "model_cnn.add(MaxPooling1D(pool_size=2))\n",
    "model_cnn.add(Flatten())\n",
    "model_cnn.add(Dense(128, activation='relu'))\n",
    "model_cnn.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile and train the CNN model\n",
    "model_cnn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model_cnn.fit(x_train_cnn, y_train_encoded, epochs=10, batch_size=32, validation_data=(x_test_cnn, y_test_encoded))\n",
    "\n",
    "# Evaluate the CNN model on the test set\n",
    "test_loss_cnn, test_accuracy_cnn = model_cnn.evaluate(x_test_cnn, y_test_encoded)\n",
    "print(f'Test Loss (CNN): {test_loss_cnn}, Test Accuracy (CNN): {test_accuracy_cnn}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34ecc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#more complex CNN by incorporating hyperparameter tuning, dropout for regularization, \n",
    "#and a more sophisticated model architecture. Code aims to find the best combination of hyperparameters \n",
    "#for improved performance.\n",
    "\n",
    "# Label encoding for y_train and y_test\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Convert the encoded labels to categorical format\n",
    "num_classes = len(label_encoder.classes_)\n",
    "y_train_categorical = to_categorical(y_train_encoded, num_classes=num_classes)\n",
    "y_test_categorical = to_categorical(y_test_encoded, num_classes=num_classes)\n",
    "\n",
    "# Reshape the data for CNN\n",
    "x_train_cnn = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))  # Adjusted input shape\n",
    "x_test_cnn = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))\n",
    "\n",
    "# Define the 1D CNN model\n",
    "def create_cnn_model(input_shape, learning_rate=0.001, dropout_rate=0.25):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(32, kernel_size=3, activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    \n",
    "    model.add(Conv1D(64, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    \n",
    "    model.add(Conv1D(128, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(Dense(10, activation='softmax'))  # Assuming 10 classes for the output layer\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Hyperparameter tuning\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "batch_sizes = [32, 64, 128]\n",
    "\n",
    "best_model = None\n",
    "best_accuracy = 0.0\n",
    "\n",
    "# Use the shape of a single example in the dataset to determine the input shape\n",
    "input_shape = x_train_cnn[0].shape\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for batch_size in batch_sizes:\n",
    "        cnn_model = create_cnn_model(input_shape, learning_rate=lr)\n",
    "        \n",
    "        # Train the model\n",
    "        cnn_history = cnn_model.fit(x_train_cnn, y_train_categorical, epochs=10, batch_size=batch_size,\n",
    "                                     validation_split=0.2, callbacks=[EarlyStopping(patience=3)])\n",
    "        \n",
    "        # Evaluate on the test set\n",
    "        test_loss, test_accuracy = cnn_model.evaluate(x_test_cnn, y_test_categorical)\n",
    "        \n",
    "        print(f\"Learning Rate: {lr}, Batch Size: {batch_size}\")\n",
    "        print(f\"Test Accuracy: {test_accuracy}\")\n",
    "        \n",
    "        # Check if this model is better\n",
    "        if test_accuracy > best_accuracy:\n",
    "            best_accuracy = test_accuracy\n",
    "            best_model = cnn_model\n",
    "\n",
    "# Use the best model for predictions or further analysis\n",
    "print(\"Best Model Summary:\")\n",
    "best_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4055b321",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LeNet-5:\n",
    "# Label encoding for y_train and y_test\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Convert the encoded labels to categorical format\n",
    "num_classes = len(label_encoder.classes_)\n",
    "y_train_categorical = to_categorical(y_train_encoded, num_classes=num_classes)\n",
    "y_test_categorical = to_categorical(y_test_encoded, num_classes=num_classes)\n",
    "\n",
    "# Reshape the data for CNN\n",
    "x_train_cnn = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))  # Adjusted input shape\n",
    "x_test_cnn = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))\n",
    "\n",
    "# Define the LeNet-5 model\n",
    "def create_lenet5_model(input_shape, learning_rate=0.001, dropout_rate=0.25):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(6, kernel_size=5, activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Conv1D(16, kernel_size=5, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(120, activation='relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(84, activation='relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(10, activation='softmax'))  # Assuming 10 classes for the output layer\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Hyperparameter tuning\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "batch_sizes = [32, 64, 128]\n",
    "\n",
    "best_model = None\n",
    "best_accuracy = 0.0\n",
    "\n",
    "# Use the shape of a single example in the dataset to determine the input shape\n",
    "input_shape = x_train_cnn[0].shape\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for batch_size in batch_sizes:\n",
    "        cnn_model = create_lenet5_model(input_shape, learning_rate=lr)\n",
    "        \n",
    "        # Train the model\n",
    "        cnn_history = cnn_model.fit(x_train_cnn, y_train_categorical, epochs=10, batch_size=batch_size,\n",
    "                                     validation_split=0.2, callbacks=[EarlyStopping(patience=3)])\n",
    "        \n",
    "        # Evaluate on the test set\n",
    "        test_loss, test_accuracy = cnn_model.evaluate(x_test_cnn, y_test_categorical)\n",
    "        \n",
    "        print(f\"Learning Rate: {lr}, Batch Size: {batch_size}\")\n",
    "        print(f\"Test Accuracy: {test_accuracy}\")\n",
    "        \n",
    "        # Check if this model is better\n",
    "        if test_accuracy > best_accuracy:\n",
    "            best_accuracy = test_accuracy\n",
    "            best_model = cnn_model\n",
    "\n",
    "# Use the best model for predictions or further analysis\n",
    "print(\"Best Model Summary:\")\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c36a19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding for y_train and y_test\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Convert the encoded labels to categorical format\n",
    "num_classes = len(label_encoder.classes_)\n",
    "y_train_categorical = to_categorical(y_train_encoded, num_classes=num_classes)\n",
    "y_test_categorical = to_categorical(y_test_encoded, num_classes=num_classes)\n",
    "\n",
    "# Reshape the data for CNN\n",
    "x_train_cnn = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))  # Adjusted input shape\n",
    "x_test_cnn = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))\n",
    "\n",
    "from keras.layers import GlobalAveragePooling1D\n",
    "\n",
    "# Define the AlexNet model\n",
    "def create_alexnet_model(input_shape, learning_rate=0.001, dropout_rate=0.25):\n",
    "    model = Sequential()\n",
    "    print(f\"Input Shape: {input_shape}\")\n",
    "    model.add(Conv1D(96, kernel_size=11, strides=1, activation='relu', input_shape=input_shape))\n",
    "    print(f\"Conv1D_1 Output Shape: {model.output_shape}\")\n",
    "    model.add(MaxPooling1D(pool_size=3, strides=1))\n",
    "    print(f\"MaxPooling1D_1 Output Shape: {model.output_shape}\")\n",
    "    model.add(Conv1D(256, kernel_size=5, activation='relu'))\n",
    "    print(f\"Conv1D_2 Output Shape: {model.output_shape}\")\n",
    "    model.add(MaxPooling1D(pool_size=3, strides=1))\n",
    "    print(f\"MaxPooling1D_2 Output Shape: {model.output_shape}\")\n",
    "    model.add(Conv1D(384, kernel_size=3, activation='relu'))\n",
    "    print(f\"Conv1D_3 Output Shape: {model.output_shape}\")\n",
    "    model.add(Conv1D(384, kernel_size=3, activation='relu'))\n",
    "    print(f\"Conv1D_4 Output Shape: {model.output_shape}\")\n",
    "    model.add(Conv1D(256, kernel_size=3, activation='relu'))\n",
    "    print(f\"Conv1D_5 Output Shape: {model.output_shape}\")\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    print(f\"GlobalAveragePooling1D Output Shape: {model.output_shape}\")\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    print(f\"Dense_1 Output Shape: {model.output_shape}\")\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    print(f\"Dense_2 Output Shape: {model.output_shape}\")\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(num_classes, activation='softmax'))  # Assuming 'num_classes' for the output layer\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Hyperparameter tuning\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "batch_sizes = [32, 64, 128]\n",
    "\n",
    "best_model = None\n",
    "best_accuracy = 0.0\n",
    "\n",
    "# Use the shape of a single example in the dataset to determine the input shape\n",
    "input_shape = x_train_cnn[0].shape\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for batch_size in batch_sizes:\n",
    "        alexnet_model = create_alexnet_model(input_shape, learning_rate=lr)\n",
    "        \n",
    "        # Train the model\n",
    "        alexnet_history = alexnet_model.fit(x_train_cnn, y_train_categorical, epochs=10, batch_size=batch_size,\n",
    "                                           validation_split=0.2, callbacks=[EarlyStopping(patience=3)])\n",
    "        \n",
    "        # Evaluate on the test set\n",
    "        test_loss, test_accuracy = alexnet_model.evaluate(x_test_cnn, y_test_categorical)\n",
    "        \n",
    "        print(f\"Learning Rate: {lr}, Batch Size: {batch_size}\")\n",
    "        print(f\"Test Accuracy: {test_accuracy}\")\n",
    "        \n",
    "        # Check if this model is better\n",
    "        if test_accuracy > best_accuracy:\n",
    "            best_accuracy = test_accuracy\n",
    "            best_model = alexnet_model\n",
    "\n",
    "# Use the best model for predictions or further analysis\n",
    "print(\"Best Model Summary:\")\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f3facc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding for y_train and y_test\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Convert the encoded labels to categorical format\n",
    "num_classes = len(label_encoder.classes_)\n",
    "y_train_categorical = to_categorical(y_train_encoded, num_classes=num_classes)\n",
    "y_test_categorical = to_categorical(y_test_encoded, num_classes=num_classes)\n",
    "\n",
    "# Reshape the data for CNN\n",
    "x_train_cnn = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))  # Adjusted input shape\n",
    "x_test_cnn = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))\n",
    "\n",
    "def create_vggnet_model(input_shape, learning_rate=0.001, dropout_rate=0.25):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(64, kernel_size=3, activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Conv1D(128, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(10, activation='softmax'))  # Assuming 10 classes for the output layer\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Hyperparameter tuning\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "batch_sizes = [32, 64, 128]\n",
    "\n",
    "best_model = None\n",
    "best_accuracy = 0.0\n",
    "\n",
    "# Use the shape of a single example in the dataset to determine the input shape\n",
    "input_shape = x_train_cnn[0].shape\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for batch_size in batch_sizes:\n",
    "        cnn_model = create_vggnet_model(input_shape, learning_rate=lr)\n",
    "        \n",
    "        # Train the model\n",
    "        cnn_history = cnn_model.fit(x_train_cnn, y_train_categorical, epochs=10, batch_size=batch_size,\n",
    "                                     validation_split=0.2, callbacks=[EarlyStopping(patience=3)])\n",
    "        \n",
    "        # Evaluate on the test set\n",
    "        test_loss, test_accuracy = cnn_model.evaluate(x_test_cnn, y_test_categorical)\n",
    "        \n",
    "        print(f\"Learning Rate: {lr}, Batch Size: {batch_size}\")\n",
    "        print(f\"Test Accuracy: {test_accuracy}\")\n",
    "        \n",
    "        # Check if this model is better\n",
    "        if test_accuracy > best_accuracy:\n",
    "            best_accuracy = test_accuracy\n",
    "            best_model = cnn_model\n",
    "\n",
    "# Use the best model for predictions or further analysis\n",
    "print(\"Best Model Summary:\")\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffbe67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_googlenet_model(input_shape, learning_rate=0.001, dropout_rate=0.25):\n",
    "    def inception_module(layer_in, f1, f2_in, f2_out, f3_in, f3_out, f4_out):\n",
    "        conv1 = Conv1D(f1, kernel_size=1, activation='relu')(layer_in)\n",
    "        conv3_1 = Conv1D(f2_in, kernel_size=1, activation='relu')(layer_in)\n",
    "        conv3_3 = Conv1D(f2_out, kernel_size=3, padding='same', activation='relu')(conv3_1)\n",
    "        conv5_1 = Conv1D(f3_in, kernel_size=1, activation='relu')(layer_in)\n",
    "        conv5_5 = Conv1D(f3_out, kernel_size=5, padding='same', activation='relu')(conv5_1)\n",
    "        pool = MaxPooling1D(pool_size=3, strides=1, padding='same')(layer_in)\n",
    "        pool_conv = Conv1D(f4_out, kernel_size=1, activation='relu')(pool)\n",
    "        layer_out = concatenate([conv1, conv3_3, conv5_5, pool_conv], axis=-1)\n",
    "        return layer_out\n",
    "\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    conv1 = Conv1D(64, kernel_size=7, strides=2, activation='relu')(input_layer)\n",
    "    pool1 = MaxPooling1D(pool_size=3, strides=2, padding='same')(conv1)\n",
    "    conv2 = Conv1D(192, kernel_size=3, activation='relu')(pool1)\n",
    "    pool2 = MaxPooling1D(pool_size=3, strides=2, padding='same')(conv2)\n",
    "\n",
    "    # Add the inception module directly to the model\n",
    "    inception1 = inception_module(pool2, 64, 128, 128, 32, 32, 32)\n",
    "    \n",
    "    pool3 = MaxPooling1D(pool_size=2, strides=2, padding='same')(inception1)\n",
    "    inception2 = inception_module(pool3, 128, 192, 192, 96, 96, 96)\n",
    "    pool4 = MaxPooling1D(pool_size=2, strides=2, padding='same')(inception2)\n",
    "    inception3 = inception_module(pool4, 192, 384, 384, 192, 192, 192)\n",
    "    inception4 = inception_module(inception3, 256, 384, 384, 192, 192, 192)\n",
    "    pool5 = MaxPooling1D(pool_size=2, strides=2, padding='same')(inception4)\n",
    "\n",
    "    flatten = Flatten()(pool5)\n",
    "    dense1 = Dense(1024, activation='relu')(flatten)\n",
    "    dropout = Dropout(dropout_rate)(dense1)\n",
    "    output_layer = Dense(10, activation='softmax')(dropout)  # Assuming 10 classes for the output layer\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Hyperparameter tuning\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "batch_sizes = [32, 64, 128]\n",
    "\n",
    "best_model = None\n",
    "best_accuracy = 0.0\n",
    "\n",
    "# Use the shape of a single example in the dataset to determine the input shape\n",
    "input_shape = x_train_cnn[0].shape\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for batch_size in batch_sizes:\n",
    "        cnn_model = create_googlenet_model(input_shape, learning_rate=lr)\n",
    "        \n",
    "        # Train the model\n",
    "        cnn_history = cnn_model.fit(x_train_cnn, y_train_categorical, epochs=10, batch_size=batch_size,\n",
    "                                     validation_split=0.2, callbacks=[EarlyStopping(patience=3)])\n",
    "        \n",
    "        # Evaluate on the test set\n",
    "        test_loss, test_accuracy = cnn_model.evaluate(x_test_cnn, y_test_categorical)\n",
    "        \n",
    "        print(f\"Learning Rate: {lr}, Batch Size: {batch_size}\")\n",
    "        print(f\"Test Accuracy: {test_accuracy}\")\n",
    "        \n",
    "        # Check if this model is better\n",
    "        if test_accuracy > best_accuracy:\n",
    "            best_accuracy = test_accuracy\n",
    "            best_model = cnn_model\n",
    "\n",
    "# Use the best model for predictions or further analysis\n",
    "print(\"Best Model Summary:\")\n",
    "best_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dfa82a",
   "metadata": {},
   "source": [
    "# 2D CNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394dc82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create and save spectrogram images\n",
    "def save_spectrogram_image(y, sr, genre, filename):\n",
    "    plt.interactive(False)\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "    plt.specgram(y, NFFT=2048, Fs=2, Fc=0, noverlap=128, cmap='inferno', sides='default', mode='default', scale='dB');\n",
    "    plt.axis('off')\n",
    "    plt.savefig(filename, dpi=100, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cd2082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing\n",
    "header = 'filename chroma_stft rmse spectral_centroid spectral_bandwidth rolloff zero_crossing_rate tempo flux contrast flatness'\n",
    "for i in range(1, 21):\n",
    "    header += f' mfcc{i}'\n",
    "header += ' label'\n",
    "header = header.split()\n",
    "file = open('data_with_image_paths.csv', 'w', newline='')\n",
    "with file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(header)\n",
    "genres = 'blues classical country disco hiphop jazz metal pop reggae rock'.split()\n",
    "for g in genres:\n",
    "    for filename in os.listdir('C:/Users/bupes/OneDrive/Desktop/project/genres_original/'+g):\n",
    "        songname = 'C:/Users/bupes/OneDrive/Desktop/project/genres_original/'+ g + '/' + filename\n",
    "        y, sr = librosa.load(songname, mono=True, duration=30)\n",
    "        chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "        spec_cent = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "        spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
    "        rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
    "        zcr = librosa.feature.zero_crossing_rate(y)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr)\n",
    "\n",
    "        # Save the spectrogram image\n",
    "        save_spectrogram_image(y, sr, g, 'C:/Users/bupes/OneDrive/Desktop/images/' + filename[:-3] + 'png')\n",
    "\n",
    "        # Append data to the CSV file\n",
    "        to_append = f'{filename} {np.mean(chroma_stft)} {np.mean(spec_cent)} {np.mean(spec_bw)} {np.mean(rolloff)} {np.mean(zcr)}'    \n",
    "        for e in mfcc:\n",
    "            to_append += f' {np.mean(e)}'\n",
    "        to_append += f' {g} {\"C:/Users/bupes/OneDrive/Desktop/images/\" + filename[:-3] + \"png\"}'\n",
    "        with open('data_with_image_paths.csv', 'a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(to_append.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628d990d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved dataset with image paths\n",
    "image_metadata = pd.read_csv(\"data_with_image_paths.csv\")\n",
    "image_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d9552b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images and convert to numpy array\n",
    "X = []\n",
    "for img_path in image_metadata['mfcc17']:\n",
    "    img = plt.imread(img_path)\n",
    "    X.append(img)\n",
    "\n",
    "X = np.array(X)\n",
    "\n",
    "# Label encoding for y_train and y_test\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(image_metadata['label'])\n",
    "y_categorical = to_categorical(y_encoded, num_classes=len(label_encoder.classes_))\n",
    "\n",
    "# Train-Test split for the dataset\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y_categorical, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f820e09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your CNN model\n",
    "def create_cnn_model(input_shape, num_classes, learning_rate=0.001, dropout_rate=0.25):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de21425",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameter tuning\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "batch_sizes = [32, 64, 128]\n",
    "\n",
    "best_model = None\n",
    "best_accuracy = 0.0\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for batch_size in batch_sizes:\n",
    "        cnn_model = create_cnn_model(input_shape=x_train[0].shape, num_classes=len(label_encoder.classes_), learning_rate=lr)\n",
    "        \n",
    "        # Train the model\n",
    "        cnn_history = cnn_model.fit(x_train, y_train, epochs=10, batch_size=batch_size,\n",
    "                                    validation_split=0.2, callbacks=[EarlyStopping(patience=3)])\n",
    "        \n",
    "        # Evaluate on the test set\n",
    "        test_loss, test_accuracy = cnn_model.evaluate(x_test, y_test)\n",
    "        \n",
    "        print(f\"Learning Rate: {lr}, Batch Size: {batch_size}\")\n",
    "        print(f\"Test Accuracy: {test_accuracy}\")\n",
    "        \n",
    "        # Check if this model is better\n",
    "        if test_accuracy > best_accuracy:\n",
    "            best_accuracy = test_accuracy\n",
    "            best_model = cnn_model\n",
    "\n",
    "# Use the best model for predictions or further analysis\n",
    "print(\"Best Model Summary:\")\n",
    "best_model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
